{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T14:46:39.637221Z",
     "start_time": "2019-11-23T14:43:57.005628Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    " \n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "\n",
    "fileName = \"../data/QSgS8RIFZQA.mp4\"\n",
    "cap = cv2.VideoCapture(fileName)\n",
    " \n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened()== False): \n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "#Read until video is completed\n",
    "while(cap.isOpened()):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Frame',frame)\n",
    "\n",
    "        # Press Q on keyboard to  exit\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Break the loop\n",
    "    else: \n",
    "        break\n",
    "\n",
    "# When everything done, release the video capture object\n",
    "cap.release()\n",
    " \n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-22T05:39:03.337968Z",
     "start_time": "2019-11-22T05:39:03.068076Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    " \n",
    "# Create a VideoCapture object and read from input file\n",
    "# If the input is the camera, pass 0 instead of the video file name\n",
    "\n",
    "fileName = \"../data/AuliiCravalho.webm\"\n",
    "def getFrames(file_name, file_output, start_frame=0, end_frame=20, step=1):\n",
    "    cap = cv2.VideoCapture(file_name)\n",
    "    success,image = cap.read()\n",
    "    count = 0\n",
    "    success = True\n",
    "    while success:\n",
    "        success,image = cap.read()\n",
    "        if count > start_frame and count%step==0:\n",
    "            cv2.imwrite(file_output+\"%d.jpg\" % count, image)     # save frame as JPEG file\n",
    "            print('Read a new frame: ', success)\n",
    "        if count>end_frame:\n",
    "            break\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T04:13:56.574386Z",
     "start_time": "2019-11-23T04:13:55.423234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n",
      "Read a new frame:  True\n"
     ]
    }
   ],
   "source": [
    "file_name_ballet1 = \"../data/Wz_f9B4pPtg.webm\"\n",
    "file_name_ballet2 = \"../data/zV1qLYukTH8.webm\"\n",
    "file_name_ballet3 = \"../data/qy6dlGpC3Ns.webm\"\n",
    "file_name_ballet4 = \"../data/XsoRgJq2c5Q.webm\"\n",
    "file_name_ballet5 = \"../data/QSgS8RIFZQA.webm\"\n",
    "\n",
    "getFrames(file_name_ballet5, \"../data/ballet/ballet5_\", 1800, 1850, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T04:20:07.314601Z",
     "start_time": "2019-11-23T04:20:07.241693Z"
    }
   },
   "outputs": [],
   "source": [
    "fileName = \"../data/Moana1.mp4\"\n",
    "getFrames(fileName, \"../data/test_\", 1800, 1825, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T15:50:26.244583Z",
     "start_time": "2019-11-23T15:50:26.240755Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2040"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "24*(60*1+25)\n",
    "#zV1qLYukTH8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T15:06:50.922582Z",
     "start_time": "2019-11-23T15:06:50.918346Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"../data/test_QSgS8RIFZQA.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video, Image, display\n",
    "Video(\"../data/test_QSgS8RIFZQA.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for pose estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T15:14:13.868672Z",
     "start_time": "2019-11-24T15:14:13.365974Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#import matplotlib\n",
    "\n",
    "from random import randint\n",
    "\n",
    "protoFile = \"../resources/pose/coco/pose_deploy_linevec.prototxt\"\n",
    "weightsFile = \"../resources/pose/coco/pose_iter_440000.caffemodel\"\n",
    "nPoints = 18\n",
    "# COCO Output Format\n",
    "keypointsMapping = ['Nose', 'Neck', 'R-Sho', 'R-Elb', 'R-Wr', 'L-Sho', \n",
    "                    'L-Elb', 'L-Wr', 'R-Hip', 'R-Knee', 'R-Ank', 'L-Hip', \n",
    "                    'L-Knee', 'L-Ank', 'R-Eye', 'L-Eye', 'R-Ear', 'L-Ear']\n",
    "\n",
    "POSE_PAIRS = [[1,2], [1,5], [2,3], [3,4], [5,6], [6,7],\n",
    "              [1,8], [8,9], [9,10], [1,11], [11,12], [12,13],\n",
    "              [1,0], [0,14], [14,16], [0,15], [15,17],\n",
    "              [2,17], [5,16] ]\n",
    "\n",
    "# index of pafs correspoding to the POSE_PAIRS\n",
    "# e.g for POSE_PAIR(1,2), the PAFs are located at indices (31,32) of output, Similarly, (1,5) -> (39,40) and so on.\n",
    "mapIdx = [[31,32], [39,40], [33,34], [35,36], [41,42], [43,44], \n",
    "          [19,20], [21,22], [23,24], [25,26], [27,28], [29,30], \n",
    "          [47,48], [49,50], [53,54], [51,52], [55,56], \n",
    "          [37,38], [45,46]]\n",
    "\n",
    "colors = [ [0,100,255], [0,100,255], [0,255,255], [0,100,255], [0,255,255], [0,100,255],\n",
    "         [0,255,0], [255,200,100], [255,0,255], [0,255,0], [255,200,100], [255,0,255],\n",
    "         [0,0,255], [255,0,0], [200,200,0], [255,0,0], [200,200,0], [0,0,0]]\n",
    "\n",
    "net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
    "\n",
    "\n",
    "def getPoseImage(image1):\n",
    "    ###########################################\n",
    "    # Find the Keypoints using Non Maximum Suppression on the Confidence Map\n",
    "    def getKeypoints(probMap, threshold=0.1):\n",
    "\n",
    "        mapSmooth = cv2.GaussianBlur(probMap,(3,3),0,0)\n",
    "\n",
    "        mapMask = np.uint8(mapSmooth>threshold)\n",
    "        keypoints = []\n",
    "\n",
    "        #find the blobs\n",
    "        _, contours, _ = cv2.findContours(mapMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        #for each blob find the maxima\n",
    "        for cnt in contours:\n",
    "            blobMask = np.zeros(mapMask.shape)\n",
    "            blobMask = cv2.fillConvexPoly(blobMask, cnt, 1)\n",
    "            maskedProbMap = mapSmooth * blobMask\n",
    "            _, maxVal, _, maxLoc = cv2.minMaxLoc(maskedProbMap)\n",
    "            keypoints.append(maxLoc + (probMap[maxLoc[1], maxLoc[0]],))\n",
    "\n",
    "        return keypoints\n",
    "\n",
    "    # Find valid connections between the different joints of a all persons present\n",
    "    def getValidPairs(output):\n",
    "        valid_pairs = []\n",
    "        invalid_pairs = []\n",
    "        n_interp_samples = 10\n",
    "        paf_score_th = 0.1\n",
    "        conf_th = 0.7\n",
    "        # loop for every POSE_PAIR\n",
    "        for k in range(len(mapIdx)):\n",
    "            # A->B constitute a limb\n",
    "            pafA = output[0, mapIdx[k][0], :, :]\n",
    "            pafB = output[0, mapIdx[k][1], :, :]\n",
    "            pafA = cv2.resize(pafA, (frameWidth, frameHeight))\n",
    "            pafB = cv2.resize(pafB, (frameWidth, frameHeight))\n",
    "\n",
    "            # Find the keypoints for the first and second limb\n",
    "            candA = detected_keypoints[POSE_PAIRS[k][0]]\n",
    "            candB = detected_keypoints[POSE_PAIRS[k][1]]\n",
    "            nA = len(candA)\n",
    "            nB = len(candB)\n",
    "\n",
    "            # If keypoints for the joint-pair is detected\n",
    "            # check every joint in candA with every joint in candB \n",
    "            # Calculate the distance vector between the two joints\n",
    "            # Find the PAF values at a set of interpolated points between the joints\n",
    "            # Use the above formula to compute a score to mark the connection valid\n",
    "\n",
    "            if( nA != 0 and nB != 0):\n",
    "                valid_pair = np.zeros((0,3))\n",
    "                for i in range(nA):\n",
    "                    max_j=-1\n",
    "                    maxScore = -1\n",
    "                    found = 0\n",
    "                    for j in range(nB):\n",
    "                        # Find d_ij\n",
    "                        d_ij = np.subtract(candB[j][:2], candA[i][:2])\n",
    "                        norm = np.linalg.norm(d_ij)\n",
    "                        if norm:\n",
    "                            d_ij = d_ij / norm\n",
    "                        else:\n",
    "                            continue\n",
    "                        # Find p(u)\n",
    "                        interp_coord = list(zip(np.linspace(candA[i][0], candB[j][0], num=n_interp_samples),\n",
    "                                                np.linspace(candA[i][1], candB[j][1], num=n_interp_samples)))\n",
    "                        # Find L(p(u))\n",
    "                        paf_interp = []\n",
    "                        for k in range(len(interp_coord)):\n",
    "                            paf_interp.append([pafA[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))],\n",
    "                                               pafB[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))] ]) \n",
    "                        # Find E\n",
    "                        paf_scores = np.dot(paf_interp, d_ij)\n",
    "                        avg_paf_score = sum(paf_scores)/len(paf_scores)\n",
    "\n",
    "                        # Check if the connection is valid\n",
    "                        # If the fraction of interpolated vectors aligned with PAF is higher then threshold -> Valid Pair  \n",
    "                        if ( len(np.where(paf_scores > paf_score_th)[0]) / n_interp_samples ) > conf_th :\n",
    "                            if avg_paf_score > maxScore:\n",
    "                                max_j = j\n",
    "                                maxScore = avg_paf_score\n",
    "                                found = 1\n",
    "                    # Append the connection to the list\n",
    "                    if found:            \n",
    "                        valid_pair = np.append(valid_pair, [[candA[i][3], candB[max_j][3], maxScore]], axis=0)\n",
    "\n",
    "                # Append the detected connections to the global list\n",
    "                valid_pairs.append(valid_pair)\n",
    "            else: # If no keypoints are detected\n",
    "                #print(\"No Connection : k = {}\".format(k))\n",
    "                invalid_pairs.append(k)\n",
    "                valid_pairs.append([])\n",
    "        #print(valid_pairs)\n",
    "        return valid_pairs, invalid_pairs\n",
    "\n",
    "    # This function creates a list of keypoints belonging to each person\n",
    "    # For each detected valid pair, it assigns the joint(s) to a person\n",
    "    # It finds the person and index at which the joint should be added. This can be done since we have an id for each joint\n",
    "    def getPersonwiseKeypoints(valid_pairs, invalid_pairs):\n",
    "        # the last number in each row is the overall score \n",
    "        personwiseKeypoints = -1 * np.ones((0, 19))\n",
    "\n",
    "        for k in range(len(mapIdx)):\n",
    "            if k not in invalid_pairs:\n",
    "                partAs = valid_pairs[k][:,0]\n",
    "                partBs = valid_pairs[k][:,1]\n",
    "                indexA, indexB = np.array(POSE_PAIRS[k])\n",
    "\n",
    "                for i in range(len(valid_pairs[k])): \n",
    "                    found = 0\n",
    "                    person_idx = -1\n",
    "                    for j in range(len(personwiseKeypoints)):\n",
    "                        if personwiseKeypoints[j][indexA] == partAs[i]:\n",
    "                            person_idx = j\n",
    "                            found = 1\n",
    "                            break\n",
    "\n",
    "                    if found:\n",
    "                        personwiseKeypoints[person_idx][indexB] = partBs[i]\n",
    "                        personwiseKeypoints[person_idx][-1] += keypoints_list[partBs[i].astype(int), 2] + valid_pairs[k][i][2]\n",
    "\n",
    "                    # if find no partA in the subset, create a new subset\n",
    "                    elif not found and k < 17:\n",
    "                        row = -1 * np.ones(19)\n",
    "                        row[indexA] = partAs[i]\n",
    "                        row[indexB] = partBs[i]\n",
    "                        # add the keypoint_scores for the two keypoints and the paf_score \n",
    "                        row[-1] = sum(keypoints_list[valid_pairs[k][i,:2].astype(int), 2]) + valid_pairs[k][i][2]\n",
    "                        personwiseKeypoints = np.vstack([personwiseKeypoints, row])\n",
    "        return personwiseKeypoints\n",
    "\n",
    "\n",
    "    ###########################################\n",
    "    frameWidth = image1.shape[1]\n",
    "    frameHeight = image1.shape[0]\n",
    "\n",
    "    t = time.time()\n",
    "    \n",
    "\n",
    "    # Fix the input Height and get the width according to the Aspect Ratio\n",
    "    inHeight = 368\n",
    "    inWidth = int((inHeight/frameHeight)*frameWidth)\n",
    "\n",
    "    inpBlob = cv2.dnn.blobFromImage(image1, 1.0 / 255, (inWidth, inHeight),\n",
    "                              (0, 0, 0), swapRB=False, crop=False)\n",
    "\n",
    "    net.setInput(inpBlob)\n",
    "    output = net.forward()\n",
    "    detected_keypoints = []\n",
    "    keypoints_list = np.zeros((0,3))\n",
    "    keypoint_id = 0\n",
    "    threshold = 0.1\n",
    "\n",
    "    for part in range(nPoints):\n",
    "        probMap = output[0,part,:,:]\n",
    "        probMap = cv2.resize(probMap, (image1.shape[1], image1.shape[0]))\n",
    "\n",
    "        keypoints = getKeypoints(probMap, threshold)\n",
    "        #print(\"Keypoints - {} : {}\".format(keypointsMapping[part], keypoints))\n",
    "        keypoints_with_id = []\n",
    "        for i in range(len(keypoints)):\n",
    "            keypoints_with_id.append(keypoints[i] + (keypoint_id,))\n",
    "            keypoints_list = np.vstack([keypoints_list, keypoints[i]])\n",
    "            keypoint_id += 1\n",
    "\n",
    "        detected_keypoints.append(keypoints_with_id)\n",
    "\n",
    "    frameClone = image1.copy()\n",
    "    for i in range(nPoints):\n",
    "        for j in range(len(detected_keypoints[i])):\n",
    "            cv2.circle(frameClone, detected_keypoints[i][j][0:2], 3, [0,0,255], -1, cv2.LINE_AA)\n",
    "    #plt.figure(figsize=[15,15])\n",
    "    #plt.imshow(frameClone[:,:,[2,1,0]])\n",
    "\n",
    "    valid_pairs, invalid_pairs = getValidPairs(output)\n",
    "    personwiseKeypoints = getPersonwiseKeypoints(valid_pairs, invalid_pairs)\n",
    "\n",
    "    for i in range(17):\n",
    "        for n in range(len(personwiseKeypoints)):\n",
    "            index = personwiseKeypoints[n][np.array(POSE_PAIRS[i])]\n",
    "            if -1 in index:\n",
    "                continue\n",
    "            B = np.int32(keypoints_list[index.astype(int), 0])\n",
    "            A = np.int32(keypoints_list[index.astype(int), 1])\n",
    "            cv2.line(frameClone, (B[0], A[0]), (B[1], A[1]), colors[i], 3, cv2.LINE_AA)\n",
    "        \n",
    "    return frameClone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T15:14:21.769967Z",
     "start_time": "2019-11-24T15:14:21.760057Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def poseDetectOnFrames(file_name, file_output, start_frame=0, end_frame=20, step=1):\n",
    "    cap = cv2.VideoCapture(file_name)\n",
    "    success,image = cap.read()\n",
    "    \n",
    "    #---\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'AVC1')\n",
    "    out = cv2.VideoWriter(file_output+\".mp4\", fourcc, 24.0, (640,  360))\n",
    "    #---\n",
    "    count = 0\n",
    "    success = True\n",
    "    while success:\n",
    "        success,image = cap.read()\n",
    "        if count > start_frame and count%step==0 and success:\n",
    "            #cv2.imwrite(file_output+\"%d.jpg\" % count, image)     # save frame as JPEG file\n",
    "            #print('Read a new frame: ', success)\n",
    "            img1 = getPoseImage(image)\n",
    "            #cv2.line(img1, (160, 180), (480, 180), [0,0,0], 3, cv2.LINE_AA)\n",
    "            out.write(img1)\n",
    "        if count>end_frame:\n",
    "            break\n",
    "        count += 1\n",
    "    print(f\"count is: {count}\")\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "def getFrames(file_name, file_output, start_frame=0, end_frame=20, step=1):\n",
    "    cap = cv2.VideoCapture(file_name)\n",
    "    success,image = cap.read()\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    #---\n",
    "    #fourcc = cv2.VideoWriter_fourcc(*'AVC1')\n",
    "    #out = cv2.VideoWriter(file_output+\".mp4\", fourcc, 24.0, (640,  360))\n",
    "    #---\n",
    "    count = 0\n",
    "    success = True\n",
    "    while success:\n",
    "        success,image = cap.read()\n",
    "        if count > start_frame and count%step==0 and success:\n",
    "            #cv2.imwrite(file_output+\"%d.jpg\" % count, image)     # save frame as JPEG file\n",
    "            #print('Read a new frame: ', success)\n",
    "            #img1 = getPoseImage(image)\n",
    "            #cv2.line(img1, (160, 180), (480, 180), [0,0,0], 3, cv2.LINE_AA)\n",
    "            #out.write(img1)\n",
    "            images.append(image)\n",
    "        if count>end_frame:\n",
    "            break\n",
    "        count += 1\n",
    "    print(f\"count is: {count}\")\n",
    "    cap.release()\n",
    "    #out.release()\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for testing post estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T16:36:57.077174Z",
     "start_time": "2019-11-24T16:35:04.610877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count is: 1301\n",
      "Time Taken = 112.4600841999054\n"
     ]
    }
   ],
   "source": [
    "test_image = \"../data/qy6dlGpC3Ns.mp4\"\n",
    "out_image = \"../data/qy6dlGpC3Ns_wAnnot\"\n",
    "\n",
    "t = time.time()\n",
    "poseDetectOnFrames(test_image, out_image, start_frame=1200, end_frame=1300, step=3)\n",
    "print(\"Time Taken = {}\".format(time.time() - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T18:14:46.961263Z",
     "start_time": "2019-11-23T18:14:46.957313Z"
    }
   },
   "source": [
    "### Code for testing multiple images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T15:22:13.934163Z",
     "start_time": "2019-11-24T15:20:47.153080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count is: 25\n",
      "<class 'numpy.ndarray'>\n",
      "Time Taken = 86.71889305114746\n",
      "<class 'cv2.dnn_Net'>\n"
     ]
    }
   ],
   "source": [
    "#test_image = \"../data/test_QSgS8RIFZQA.mp4\"\n",
    "#out_image = \"../data/test_QSgS8RIFZQA_wAnnot\"\n",
    "\n",
    "t = time.time()\n",
    "# Step1: Get images\n",
    "test_imgs = getFrames(test_image, out_image, start_frame=0, end_frame=24, step=1)\n",
    "\n",
    "# Step2: Pass images through the net:\n",
    "frameWidth = test_imgs[0].shape[1]\n",
    "frameHeight = test_imgs[0].shape[0]\n",
    "\n",
    "t = time.time()\n",
    "    \n",
    "\n",
    "# Fix the input Height and get the width according to the Aspect Ratio\n",
    "inHeight = 368\n",
    "inWidth = int((inHeight/frameHeight)*frameWidth)\n",
    "\n",
    "inpBlob = cv2.dnn.blobFromImages(test_imgs, 1.0 / 255, (inWidth, inHeight),\n",
    "                                (0, 0, 0), swapRB=False, crop=False)\n",
    "\n",
    "net.setInput(inpBlob)\n",
    "output = net.forward()\n",
    "print(type(output))\n",
    "print(\"Time Taken = {}\".format(time.time() - t))\n",
    "\n",
    "print(type(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T15:43:34.377525Z",
     "start_time": "2019-11-24T15:43:34.372482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 57, 46, 82)\n",
      "(25, 3, 368, 654)\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)\n",
    "print(inpBlob.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
